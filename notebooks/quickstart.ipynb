{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# detr-geo Quickstart: Vehicle Detection from Satellite Imagery\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gpriceless/detr-geo/blob/main/notebooks/quickstart.ipynb)\n[![GitHub](https://img.shields.io/badge/github-detr--geo-blue?logo=github)](https://github.com/gpriceless/detr-geo)\n\n**Detect vehicles in satellite and aerial imagery, export georeferenced vector data.**\n\n[detr-geo](https://github.com/gpriceless/detr-geo) wraps RF-DETR and adds everything a geospatial workflow needs: automatic tiling, CRS handling, multispectral band mapping, and export to GeoJSON/GeoPackage/Shapefile.\n\nThis notebook walks through the full workflow:\n\n1. Install detr-geo and dependencies\n2. Download a real satellite image (NAIP imagery from USDA)\n3. Run the **COCO-pretrained model** -- see how it struggles with overhead imagery\n4. Run the **xView fine-tuned model** -- see accurate vehicle detection from above\n5. Visualize results and export georeferenced data\n\n**Runtime**: ~5 minutes on a free Colab GPU (T4)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install detr-geo\n",
    "\n",
    "This installs the library with all dependencies: RF-DETR, PyTorch, rasterio, geopandas, matplotlib, and leafmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install detr-geo with all optional dependencies\n",
    "!pip install \"detr-geo[all] @ git+https://github.com/gpriceless/detr-geo.git\"\n",
    "!pip install pystac-client  # For searching satellite imagery catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detr_geo\n",
    "print(f\"detr-geo {detr_geo.__version__} installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Sample Satellite Imagery\n",
    "\n",
    "We will fetch a crop of **NAIP** (National Agriculture Imagery Program) data -- public domain aerial imagery covering the entire US at **0.6m resolution**. Perfect for vehicle detection.\n",
    "\n",
    "We use Microsoft's [Planetary Computer](https://planetarycomputer.microsoft.com/) STAC catalog to find a scene over a large parking lot, then read a small window with rasterio. No API key required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pystac_client import Client\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "\n",
    "# Target: a Costco parking lot in Fresno, CA\n",
    "# Large, well-organized lot with a mix of cars, trucks, and empty spaces.\n",
    "TARGET_LON, TARGET_LAT = -119.7895, 36.8385\n",
    "\n",
    "# Search for NAIP imagery at this location\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"naip\"],\n",
    "    intersects={\"type\": \"Point\", \"coordinates\": [TARGET_LON, TARGET_LAT]},\n",
    "    datetime=\"2020-01-01/2023-12-31\",\n",
    "    max_items=5,\n",
    ")\n",
    "\n",
    "items = list(search.items())\n",
    "print(f\"Found {len(items)} NAIP scenes\")\n",
    "\n",
    "# Pick the most recent scene\n",
    "item = sorted(items, key=lambda x: x.datetime, reverse=True)[0]\n",
    "print(f\"Using: {item.id} ({item.datetime.strftime('%Y-%m-%d')})\")\n",
    "print(f\"GSD: {item.properties.get('gsd', 'unknown')}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a small crop around the parking lot\n",
    "# NAIP scenes are huge (10k+ pixels), so we window-read a ~1000x1000 pixel area\n",
    "\n",
    "# Define a bounding box: ~600m x 600m centered on the parking lot\n",
    "HALF_SIZE_DEG = 0.003  # ~300m at this latitude\n",
    "bbox = box(\n",
    "    TARGET_LON - HALF_SIZE_DEG,\n",
    "    TARGET_LAT - HALF_SIZE_DEG,\n",
    "    TARGET_LON + HALF_SIZE_DEG,\n",
    "    TARGET_LAT + HALF_SIZE_DEG,\n",
    ")\n",
    "\n",
    "# Get the image URL from the STAC item\n",
    "image_url = item.assets[\"image\"].href\n",
    "print(f\"Reading from: {image_url[:80]}...\")\n",
    "\n",
    "# Read the windowed region and save as a local GeoTIFF\n",
    "output_path = \"sample_parking_lot.tif\"\n",
    "\n",
    "with rasterio.open(image_url) as src:\n",
    "    # Convert geographic bounds to pixel window\n",
    "    window = from_bounds(*bbox.bounds, transform=src.transform)\n",
    "    \n",
    "    # Read RGB bands (NAIP has 4 bands: R, G, B, NIR)\n",
    "    data = src.read([1, 2, 3], window=window)\n",
    "    transform = src.window_transform(window)\n",
    "    \n",
    "    # Write cropped GeoTIFF\n",
    "    profile = src.profile.copy()\n",
    "    profile.update(\n",
    "        width=data.shape[2],\n",
    "        height=data.shape[1],\n",
    "        count=3,\n",
    "        transform=transform,\n",
    "        driver=\"GTiff\",\n",
    "        compress=\"deflate\",\n",
    "    )\n",
    "    with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "        dst.write(data)\n",
    "\n",
    "print(f\"Saved {output_path}: {data.shape[2]}x{data.shape[1]} pixels, {data.shape[0]} bands\")\n",
    "print(f\"File size: {Path(output_path).stat().st_size / 1024:.0f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View the Satellite Image\n",
    "\n",
    "Let's see what we're working with. This is a parking lot as seen from above at 0.6m resolution -- each pixel covers about 2 feet on the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with rasterio.open(output_path) as src:\n",
    "    img = src.read([1, 2, 3])  # (3, H, W)\n",
    "\n",
    "# Display as RGB\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.imshow(np.transpose(img, (1, 2, 0)))  # (H, W, 3)\n",
    "ax.set_title(\"NAIP Aerial Imagery -- Parking Lot (0.6m GSD)\", fontsize=14)\n",
    "ax.set_xlabel(f\"{img.shape[2]} x {img.shape[1]} pixels\")\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. COCO-Pretrained Model (The Problem)\n",
    "\n",
    "The default RF-DETR model was trained on **COCO** -- a dataset of ground-level photographs. It has never seen a car from above.\n",
    "\n",
    "Watch what happens when we point it at satellite imagery: vehicles get labeled as *motorcycles*, *skateboards*, *boats*, and other nonsensical classes. The model sees vaguely rectangular blobs and guesses the closest COCO category it knows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr_geo import DetrGeo\n",
    "\n",
    "# Load the standard COCO-pretrained model\n",
    "dg_coco = DetrGeo(model_size=\"base\", confidence_threshold=0.3)\n",
    "dg_coco.set_image(output_path, suppress_gsd_warning=True)\n",
    "\n",
    "# Run detection\n",
    "coco_detections = dg_coco.detect(threshold=0.3)\n",
    "print(f\"COCO model found {len(coco_detections)} objects\")\n",
    "\n",
    "if len(coco_detections) > 0:\n",
    "    print(\"\\nClass distribution (COCO labels):\")\n",
    "    print(coco_detections[\"class_name\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confused COCO detections\n",
    "if len(coco_detections) > 0:\n",
    "    fig, ax = dg_coco.show_detections(figsize=(14, 12))\n",
    "    ax.set_title(\n",
    "        \"COCO-Pretrained Model: Confused by Overhead Perspective\\n\"\n",
    "        \"(motorcycles? skateboards? boats? These are cars.)\",\n",
    "        fontsize=13,\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No detections -- the COCO model cannot recognize overhead vehicles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COCO model sees top-down vehicle shapes and tries to match them to ground-level categories. It is confidently wrong. This is the fundamental problem that fine-tuning solves.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. xView Fine-Tuned Model (The Solution)\n",
    "\n",
    "The **xView fine-tuned model** was trained on the [xView dataset](http://xviewdataset.org/) -- satellite imagery at 0.3m GSD with labeled overhead vehicles. It detects 5 classes:\n",
    "\n",
    "| Class | Examples |\n",
    "|---|---|\n",
    "| Car | Sedans, SUVs, hatchbacks |\n",
    "| Pickup Truck | Pickup trucks, utility pickups |\n",
    "| Truck | Semi trucks, cargo trucks, tankers |\n",
    "| Bus | Transit buses, school buses |\n",
    "| Other Vehicle | Construction equipment, specialty vehicles |\n",
    "\n",
    "First, download the fine-tuned weights from HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download xView fine-tuned weights (~100 MB)\n",
    "!pip install -q huggingface_hub\n",
    "!huggingface-cli download gpriceless/detr-geo-xview checkpoint_best_ema.pth --local-dir checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xView vehicle class mapping (must match training configuration)\n",
    "XVIEW_CLASSES = {\n",
    "    0: \"Car\",\n",
    "    1: \"Pickup Truck\",\n",
    "    2: \"Truck\",\n",
    "    3: \"Bus\",\n",
    "    4: \"Other Vehicle\",\n",
    "}\n",
    "\n",
    "# Load the xView fine-tuned model\n",
    "dg = DetrGeo(\n",
    "    model_size=\"medium\",\n",
    "    pretrain_weights=\"checkpoints/checkpoint_best_ema.pth\",\n",
    "    custom_class_names=XVIEW_CLASSES,\n",
    "    confidence_threshold=0.3,\n",
    ")\n",
    "\n",
    "dg.set_image(output_path, suppress_gsd_warning=True)\n",
    "\n",
    "# Run detection\n",
    "detections = dg.detect(threshold=0.3)\n",
    "print(f\"xView model found {len(detections)} vehicles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize xView Detections\n",
    "\n",
    "The fine-tuned model correctly identifies vehicle types from above. Compare this to the COCO results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the xView detections with bounding boxes\n",
    "fig, ax = dg.show_detections(figsize=(14, 12))\n",
    "ax.set_title(\n",
    "    f\"xView Fine-Tuned Model: {len(detections)} Vehicles Detected\\n\"\n",
    "    \"Correct classes: Car, Pickup Truck, Truck, Bus, Other Vehicle\",\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Before vs. After Comparison\n\nScroll up to compare the COCO model (Section 4) with the xView model (Section 6). The difference is dramatic:\n\n- **COCO model**: Labels overhead vehicles as motorcycles, skateboards, boats, and other nonsensical ground-level categories\n- **xView model**: Correctly identifies Car, Pickup Truck, Truck, Bus, and Other Vehicle from the overhead perspective\n\nThe table below summarizes the difference:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Quick comparison table\nprint(\"=\" * 55)\nprint(\"  MODEL COMPARISON\")\nprint(\"=\" * 55)\nprint(f\"  {'Metric':<25s} {'COCO':>12s} {'xView':>12s}\")\nprint(\"-\" * 55)\nprint(f\"  {'Total detections':<25s} {len(coco_detections):>12d} {len(detections):>12d}\")\n\nif len(coco_detections) > 0:\n    n_coco_classes = coco_detections[\"class_name\"].nunique()\n    coco_top = coco_detections[\"class_name\"].value_counts().index[0]\nelse:\n    n_coco_classes = 0\n    coco_top = \"N/A\"\n\nif len(detections) > 0:\n    n_xview_classes = detections[\"class_name\"].nunique()\n    xview_top = detections[\"class_name\"].value_counts().index[0]\nelse:\n    n_xview_classes = 0\n    xview_top = \"N/A\"\n\nprint(f\"  {'Unique classes':<25s} {n_coco_classes:>12d} {n_xview_classes:>12d}\")\nprint(f\"  {'Top class':<25s} {coco_top:>12s} {xview_top:>12s}\")\nprint(f\"  {'Correct labels?':<25s} {'No':>12s} {'Yes':>12s}\")\nprint(\"=\" * 55)\n\nif len(coco_detections) > 0:\n    print(f\"\\nCOCO classes found: {', '.join(coco_detections['class_name'].unique())}\")\nif len(detections) > 0:\n    print(f\"xView classes found: {', '.join(detections['class_name'].unique())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Class Vehicle Counts\n",
    "\n",
    "A summary of what the xView model detected, broken down by vehicle type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(detections) > 0:\n",
    "    # Per-class summary\n",
    "    counts = detections[\"class_name\"].value_counts()\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(\"  VEHICLE DETECTION SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    for cls, count in counts.items():\n",
    "        cls_data = detections[detections[\"class_name\"] == cls]\n",
    "        avg_conf = cls_data[\"confidence\"].mean()\n",
    "        print(f\"  {cls:<15s}  {count:>4d}  (avg conf: {avg_conf:.2f})\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  {'TOTAL':<15s}  {len(detections):>4d}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Confidence distribution\n",
    "    scores = detections[\"confidence\"]\n",
    "    print(f\"\\nConfidence stats:\")\n",
    "    print(f\"  Min:  {scores.min():.3f}\")\n",
    "    print(f\"  Mean: {scores.mean():.3f}\")\n",
    "    print(f\"  Max:  {scores.max():.3f}\")\n",
    "else:\n",
    "    print(\"No vehicles detected. Try lowering the threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of vehicle counts by class\n",
    "if len(detections) > 0:\n",
    "    counts = detections[\"class_name\"].value_counts()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(counts)))\n",
    "    bars = ax.barh(counts.index, counts.values, color=colors, edgecolor=\"#333333\", linewidth=0.5)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, val in zip(bars, counts.values):\n",
    "        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height() / 2,\n",
    "                str(val), va=\"center\", fontweight=\"bold\", fontsize=11)\n",
    "    \n",
    "    ax.set_xlabel(\"Count\", fontsize=12)\n",
    "    ax.set_title(\"Detected Vehicles by Class\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Georeferenced Results\n",
    "\n",
    "Detection results are a **GeoDataFrame** -- each bounding box is a polygon with real-world coordinates in the raster's CRS. Export to GeoPackage or GeoJSON for use in QGIS, ArcGIS, PostGIS, or any GIS tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The detection results are a standard GeoDataFrame\n",
    "print(f\"CRS: {detections.crs}\")\n",
    "print(f\"Columns: {list(detections.columns)}\")\n",
    "print()\n",
    "detections.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to GeoPackage (recommended -- preserves CRS, supports layers)\n",
    "dg.to_gpkg(\"vehicle_detections.gpkg\")\n",
    "print(f\"Exported: vehicle_detections.gpkg ({Path('vehicle_detections.gpkg').stat().st_size / 1024:.0f} KB)\")\n",
    "\n",
    "# Export to GeoJSON (auto-reprojects to WGS84 per the GeoJSON spec)\n",
    "dg.to_geojson(\"vehicle_detections.geojson\")\n",
    "print(f\"Exported: vehicle_detections.geojson ({Path('vehicle_detections.geojson').stat().st_size / 1024:.0f} KB)\")\n",
    "\n",
    "print(\"\\nThese files can be opened directly in QGIS, ArcGIS, or loaded into PostGIS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Map\n",
    "\n",
    "View detections on an interactive satellite basemap. Click on any detection to see its class and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive map with satellite basemap\n",
    "m = dg.show_map(basemap=\"SATELLITE\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook we demonstrated the full detr-geo workflow:\n",
    "\n",
    "| Step | Code | What it does |\n",
    "|------|------|--------------|\n",
    "| Load model | `DetrGeo(model_size=\"medium\", pretrain_weights=..., custom_class_names=...)` | Initialize with xView fine-tuned weights |\n",
    "| Load image | `dg.set_image(\"scene.tif\")` | Read CRS and transform from GeoTIFF |\n",
    "| Detect | `dg.detect(threshold=0.3)` | Run inference, return GeoDataFrame |\n",
    "| Visualize | `dg.show_detections()` | Matplotlib bounding boxes on imagery |\n",
    "| Export | `dg.to_gpkg(\"out.gpkg\")` | Georeferenced vector data for GIS |\n",
    "| Map | `dg.show_map()` | Interactive leafmap with satellite basemap |\n",
    "\n",
    "For **large rasters** (orthomosaics, full satellite scenes), use `detect_tiled()` instead of `detect()`. It automatically tiles the image, runs detection on each tile, and merges results with cross-tile NMS:\n",
    "\n",
    "```python\n",
    "detections = dg.detect_tiled(overlap=0.2, nms_threshold=0.5, threshold=0.3)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune on your own data**: See the [Fine-Tuning Guide](https://github.com/gpriceless/detr-geo/blob/main/docs/fine-tuning-guide.md)\n",
    "- **More examples**: See the [examples/](https://github.com/gpriceless/detr-geo/tree/main/examples) directory\n",
    "- **API Reference**: See the [full docs](https://github.com/gpriceless/detr-geo/blob/main/docs/api-reference.md)\n",
    "\n",
    "---\n",
    "\n",
    "*detr-geo is MIT licensed. xView fine-tuned weights are CC BY-NC-SA 4.0 (following the xView dataset license).*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}